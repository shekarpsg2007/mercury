package com.humedica.mercury.etl.core.engine

import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.{DataFrame, Row}
import org.slf4j.{Logger, LoggerFactory}

@deprecated("Do not use this class. We're keeping this class for existing utility code compatibility. Functions will be relocated since some are ETL code line specific!.")
object Metrics {

  val log: Logger = LoggerFactory.getLogger(getClass)

  // Unpivot a one-row data-frame to two columns: COLUMN and "name" (param)
  private def unpivotDF(df: DataFrame, name: String): DataFrame = {

    val ccCol = "UNPIVOT_CONCAT"
    val exCol = "UNPIVOT_EXPLODE"
    val cols = df.columns

    val df0 = df.withColumn(ccCol, concat_ws("@", cols.map { c => concat_ws("~", lit(c), df(c)) }: _*)).select(ccCol)
    val df1 = df0.withColumn(exCol, explode(split(df0(ccCol), "@")))
    val df2 = df1.withColumn("COLUMN", split(df1(exCol), "~")(0))
      .withColumn(name, split(df1(exCol), "~")(1)).select("COLUMN", name)
    /*
    df.show(10, false)
    df0.show(10, false)
    df1.show(10, false)
    df2.show(10, false)
    */
    df2
  }

  def topvals(df: DataFrame, column: String, N: Int): List[Any] = {
    if (df.columns.contains(column))
      df.groupBy(column).count.orderBy(col("count").desc).limit(N).select(column).rdd.map(r => r(0)).collect().toList
    else List()
  }

  // Compute precision & recall for each spark-generated column relative to the cdr-generated column
  // using N most common values
  def precisionRecall(cdr: DataFrame, spk: DataFrame, N: Int, p: Int = 2): DataFrame = {
    val sharedColumns = cdr.columns intersect spk.columns // They should be the same
    val data = cdr.select(sharedColumns.map(c => lit({
      val ctop = topvals(cdr, c, N)
      val stop = topvals(spk, c, N)
      val overlap = (ctop intersect stop).length.toDouble
      val precision = if (stop.length == 0) 0.0 else Engine.roundAt(p)(overlap / stop.length)
      val recall = if (ctop.length == 0) 0.0 else Engine.roundAt(p)(overlap / ctop.length)
      precision + "%" + recall
    }).alias(c)): _*).limit(1)

    unpivotDF(data, "PR").withColumn("PRECISION", split(col("PR"), "%")(0))
      .withColumn("RECALL", split(col("PR"), "%")(1))
      .drop("PR")
  }

  // non-null counts for every column
  def counts(df: DataFrame): DataFrame =
    unpivotDF(df.select(df.columns.map(c => count(col(c)).alias(c)): _*), "COUNT")

  // distinct counts for every column
  def distincts(df: DataFrame, approxDistinct: Boolean = false): DataFrame =
    unpivotDF(df.select(df.columns.map(c => if (approxDistinct) approxCountDistinct(col(c)).alias(c) else countDistinct(col(c)).alias(c)): _*), "DISTINCT")

  // # uniquely occurring values in each column
  def uniques(df: DataFrame): DataFrame =
    unpivotDF(df.select(df.columns.map(c => lit(df.groupBy(c).count.filter("count=1").count).alias(c)): _*).limit(1), "UNIQUE")

  // Most frequent N values.   (Summary is currently hardcoded for N=5)
  def common(df: DataFrame, N: Int): DataFrame =
    unpivotDF(df.select(df.columns.map(c => lit(topvals(df, c, N).mkString(",")).alias(c)): _*).limit(1), "COMMON")

  // summary for one dataframe
  def summary(df: DataFrame, tag: String = "", includeDistincts: Boolean = true, approxDistinct: Boolean = false, includeUniques: Boolean = true, includeCommon: Boolean = true, commonCount: Int = 5): DataFrame = {
    val sum0 = counts(df)
    val sum1 = if (includeDistincts) sum0.join(distincts(df, approxDistinct), "COLUMN") else sum0
    val sum2 = if (includeUniques) sum1.join(uniques(df), "COLUMN") else sum1
    val sum3 = if (includeCommon) sum2.join(common(df, commonCount), "COLUMN") else sum2
    sum3.toDF(sum3.columns.map(c => c + tag): _*).withColumnRenamed("COLUMN" + tag, "COLUMN")
  }

  // Quick Report
  def quickReport(cdr: DataFrame, spk: DataFrame, approx: Boolean = true): DataFrame =
    report(cdr, spk, approx = approx, includeUniques = false, includePrecisionRecall = false, includeCommon = false)

  // Comparison report for two dataframes, e.g, one generated by the cdr, one by spark
  def report(cdr: DataFrame, spk: DataFrame, aTag: String = "_CDR", bTag: String = "",
             includeDistincts: Boolean = true, approx: Boolean = false, includeUniques: Boolean = true,
             includeCommon: Boolean = true, commonCount: Int = 5,
             includePrecisionRecall: Boolean = true, precisionRecallTopValues: Int = 100, precisionRecallDecimalPlaces: Int = 4,
             includeDifferences: Boolean = true, includePercentDifferences: Boolean = true): DataFrame = {
    log.info("Full comparison report")
    val cdrsum = summary(cdr, aTag, includeDistincts, approx, includeUniques, includeCommon, commonCount)
    val spksum = summary(spk, bTag, includeDistincts, approx, includeUniques, includeCommon, commonCount)

    val report0 = cdrsum.join(spksum, Seq("COLUMN"), "outer")
      .withColumn("COUNT_DIFF", (col("COUNT" + bTag) - col("COUNT" + aTag)).cast("Int"))
      .withColumn("COUNT_%DIFF", when(col("COUNT" + aTag) === 0.0, lit(0.0)).otherwise(round((col("COUNT" + bTag) - col("COUNT" + aTag)) * 100.0 / col("COUNT" + aTag).cast("Double"), 1)))

    val report1 = if (includeDistincts) {
      report0.withColumn("DISTINCT_DIFF", (col("DISTINCT" + bTag) - col("DISTINCT" + aTag)).cast("Int"))
        .withColumn("DISTINCT_%DIFF", when(col("DISTINCT" + aTag) === 0.0, lit(0.0)).otherwise(round((col("DISTINCT" + bTag) - col("DISTINCT" + aTag)) * 100.0 / col("DISTINCT" + aTag).cast("Double"), 1)))
    } else report0

    val report2 = if (includeUniques) {
      report1.withColumn("UNIQUE_DIFF", (col("UNIQUE" + bTag) - col("UNIQUE" + aTag)).cast("Int"))
        .withColumn("UNIQUE_%DIFF", when(col("UNIQUE" + aTag) === 0.0, lit(0.0)).otherwise(round((col("UNIQUE" + bTag) - col("UNIQUE" + aTag)) * 100.0 / col("UNIQUE" + aTag).cast("Double"), 1)))
    } else report1

    val report3 = if (includePrecisionRecall) report2.join(precisionRecall(cdr, spk, precisionRecallTopValues, precisionRecallDecimalPlaces), Seq("COLUMN"), "outer") else report2

    // Determine which columns we actually have, and select them
    val allColumns = List("COLUMN", "COUNT" + aTag, "COUNT" + bTag, "COUNT_DIFF", "COUNT_%DIFF", "DISTINCT" + aTag, "DISTINCT" + bTag, "DISTINCT_DIFF", "DISTINCT_%DIFF", "UNIQUE" + aTag, "UNIQUE" + bTag, "UNIQUE_DIFF", "UNIQUE_%DIFF", "PRECISION", "RECALL", "COMMON" + aTag, "COMMON" + bTag)
    val filterDiffs = if (includeDifferences) allColumns else allColumns.filter(c => c.contains("_DIFF") == false)
    val filtered = if (includePercentDifferences) filterDiffs else filterDiffs.filter(c => c.contains("_%DIFF") == false)
    val selectedColumns = filtered intersect report3.columns
    report3.select(selectedColumns.head, selectedColumns.tail: _*)
  }

  def deltaReport(cdr: DataFrame, spk: DataFrame, minDiff: Int = 1, aTag: String = "_CDR", bTag: String = ""): DataFrame = {
    val cdrx = cdr.columns.map(c => cdr.groupBy(c).count.alias(c).withColumnRenamed(c, "VALUE").withColumn("COLUMN", lit(c))).toList.reduceLeft((x, y) => x unionAll y).withColumnRenamed("count", "COUNT" + aTag)
    val spkx = spk.columns.map(c => spk.groupBy(c).count.alias(c).withColumnRenamed(c, "VALUE").withColumn("COLUMN", lit(c))).toList.reduceLeft((x, y) => x unionAll y).withColumnRenamed("count", "COUNT" + bTag)
    val j = cdrx.join(spkx, Seq("COLUMN", "VALUE"), "outer")

    val k = j.withColumn("ZCOUNT" + aTag, when(j("COUNT" + aTag).isNull, 0).otherwise(j("COUNT" + aTag)))
      .withColumn("ZCOUNT" + bTag, when(j("COUNT" + bTag).isNull, 0).otherwise(j("COUNT" + bTag)))

    val m = k.withColumn("DIFF", (k("ZCOUNT" + bTag) - k("ZCOUNT" + aTag)).cast("Int"))

    val dr = m.withColumn("ABSDIFF", abs(m("DIFF"))).drop("COUNT" + aTag).drop("COUNT" + bTag)
      .withColumnRenamed("ZCOUNT" + aTag, "COUNT" + aTag)
      .withColumnRenamed("ZCOUNT" + bTag, "COUNT" + bTag)

    dr.filter("ABSDIFF >= " + minDiff).orderBy(dr("COLUMN"), dr("ABSDIFF").desc)
  }

  def simpleFdrCompare(fdrPath: String, cdrPath: Seq[String] = Seq(), reportPath: String = ""): DataFrame = {
    val fdr = readDF(fdrPath)

    val fdrCount: Long = fdr.count
    var cdrCount: Long = 0
    for (cdr <- cdrPath) {
      val thisCdr = readDF(cdr)
      cdrCount += thisCdr.count()
    }

    val countDiff: Long = fdrCount - cdrCount
    val doubleCountDiff = countDiff.toDouble
    val doubleCdrCount = cdrCount.toDouble
    val percentError: Long = ((doubleCountDiff / doubleCdrCount) * 100).toLong
    log.info("FDR count: " + fdrCount)
    log.info("CDR count: " + cdrCount)
    log.info("Count difference between FDR and CDR: " + countDiff)
    //approximate
    log.info("% Error: " + percentError + "%")

    //create dataframe for report
    val reportTable = Seq(
      Row("FDR", fdrCount),
      Row("CDR", cdrCount),
      Row("Diff", countDiff),
      Row("%Error", percentError)
    )

    val reportSchema = List(
      StructField("Type", StringType, true),
      StructField("Count", LongType, true)
    )

    val rep = Engine.spark.createDataFrame(Engine.spark.sparkContext.parallelize(reportTable), StructType(reportSchema))

    if (reportPath != "") {
      log.info("Writing to path: " + reportPath)
      rep.write.parquet(reportPath)
    }
    rep
  }

  //Could be expanded to include additional comparisons besides counts
  def fdrCompare(fdrPath: String, cdr1Path: String, cdr2Path: String = "", cdr3Path: String = ""): (DataFrame, DataFrame) = {
    val fdrPathLower = fdrPath.toLowerCase
    val cdr1PathLower = cdr1Path.toLowerCase
    val cdr2PathLower = cdr2Path.toLowerCase()
    val cdr3PathLower = cdr3Path.toLowerCase()

    val fdr = readDF(fdrPath)

    val cdrDf: DataFrame = {
      if (fdrPathLower.contains("diagnosis")) {
        val cdrDiag = readDF(cdr1Path)
        cdrDiagnosisRename(cdrDiag)
      }
      else if (fdrPathLower.contains("assessment")) {
        val cdrObsDf: DataFrame = {
          if (cdr1PathLower.contains("observation")) {
            readDF(cdr1Path)
          } else {
            readDF(cdr2Path)
          }
        }
        val cdrLabResDf: DataFrame = {
          if (cdr1PathLower.contains("labresult")) {
            readDF(cdr1Path)
          } else {
            readDF(cdr2Path)
          }
        }
        cdrAssessmentRename(cdrObsDf, cdrLabResDf, fdr)
      }
      else
        readDF(cdr1Path)
    }
    val cdrCount = cdrDf.count()
    val fdrCount = fdr.count()

    log.info("Total rows in FDR: " + fdrCount)
    log.info("Total rows in CDR: " + cdrCount)
    var diffCount: Long = 0
    if (cdrCount > fdrCount) {
      diffCount = cdrCount - fdrCount
    } else if (fdrCount > cdrCount) {
      diffCount = fdrCount - cdrCount
    }
    log.info("Difference between counts: " + diffCount)
    (fdr, fdr)
  }

  def readDF(rootPath: String): DataFrame = {
    Engine.spark.read.parquet(rootPath)
  }

  def cdrDiagnosisRename(cdr: DataFrame): DataFrame = {
    cdr.withColumnRenamed("GRP_MPI", "MPI")
      .withColumnRenamed("DX_TIMESTAMP", "DIAG_DT")
      .withColumnRenamed("CODETYPE", "CODE_TYPE")
      .withColumnRenamed("MAPPEDDIAGNOSIS", "DIAG_CD")
  }

  //delete?
  def fdrDiagnosisDrop(fdr: DataFrame): DataFrame = {
    fdr.drop("PROBLEM_LIST_IND")
  }

  def cdrAssessmentRename(cdrObs: DataFrame, cdrLabRes: DataFrame, fdr: DataFrame): DataFrame = {
    val cdrObsRename = cdrObs.withColumnRenamed("GRP_MPI", "MPI")
      .withColumnRenamed("OBSDATE", "ASSESSMENT_DTM")
      .withColumnRenamed("OBSTYPE", "ASSESSMENT_TYPE")
      .withColumnRenamed("LOCALRESULT", "TEXT_VALUE")
      .withColumnRenamed("OBSRESULT", "NUMERIC_VALUE")
    val cdrLabResRename = cdrLabRes.withColumnRenamed("GRP_MPI", "MPI")
      .withColumnRenamed("DATECOLLECTED", "ASSESSMENT_DTM")
      .withColumnRenamed("DATEAVAILALBE", "ASSESSMENT_DTM")
      .withColumnRenamed("MAPPEDCODE", "ASSESSMENT_TYPE")
      .withColumnRenamed("LOCALRESULT", "TEXT_VALUE")
      .withColumnRenamed("NORMALIZEDVALUE", "NUMERIC_VALUE")

    val fdrDrop = fdr.drop("QUALITATIVE_VALUE", "SOURCE")
    val fdrCol = fdrDrop.columns
    val cdrLabResDrop = cdrLabResRename.select(fdrCol.head, fdrCol.tail: _*)
    val cdrObsDrop = cdrObsRename.select(fdrCol.head, fdrCol.tail: _*)

    cdrObsDrop.union(cdrLabResDrop)
  }
}
